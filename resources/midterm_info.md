Midterm Information 
=

**Time**/**Date**: 14. March from 3:00-4:15pm in **TBA Location**

Overview and Rules  
--------
- The exam covers everything we have done in class up to and including Support Vector Machines.  This includes all material presented in lecture, in-class discussion, in-class notebooks, and material introduced in homework. 
- You are allowed one 8-1/2 x 11in sheet of **handwritten** notes (both sides).  No magnifying glasses! 
- You may use a calculator provided that it cannot access the internet or store large amounts of data. 
- The exam will be a mixture of multiple choice questions and free-response questions in which you may be asked to work through simple examples of algorithms or proofs. 



Material Overview 
---

**General**
- differences between classification and regression 
- differences between supervised and unsupervised learning 
- basic probability 

**K-Nearest Neighbors**
- how the algorithm works 
- how to perform classification with the algorithm 
- properties of the algorithm 

**Naive Bayes**
- assumptions behind the algorithm 
- how to compute probabilities from training data 
- how to perform classification with the algorithm 
- Laplace smoothing and it's variants 

**Logistic Regression**
- assumptions behind the algorithm 
- it's probabilistic interpretation 
- how to perform classification with the algorithm 
- how to find the weights using Stochastic Gradient Descent 
- how to efficiently perform regularization in the context of SGD 

**Validation and Evaluation Metrics** 
- cross-validation 
- confusion matrices 
- TPR and FPR 
- ROC curves 

**Multi-class Classification**
- one-vs-all classification 
- all-pairs classification 
- error correcting output codes 

**Regression**
- probabilistic interpretation of regression models 
- generalities of how weights are estimated 
- interpretation of the weights in regression models 

**Regularization** 
- the point of regularization 
- Ridge (L2) Regression 
- LASSO (L1) Regression 
- Ridge vs LASSO regression 
- the effect on bias/variance 

**Learning Theory**
- the Bias/Variance Trade-Off
- overfitting, generalization, etc 
- finite vs infinite hypothesis classes 
- PAC Learnability: what it means, what is involved, simple bounds (but no proofs) 
- VC Dimension: how to compute it (possibly simple proofs), why it matters 

**Support Vector Machines** 
- the concept of a margin 
- the geometry behind SVM 
- the workings of hard-margin SVM 
- the workings of soft-margin SVM 
- what a support vector is 
- the SMO algorithm for determining the model 
- ideas related to different kernels 




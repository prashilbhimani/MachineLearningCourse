{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7: Feature Engineering \n",
    "***\n",
    "\n",
    "<img src=\"figs/logregwordcloud.png\" width=1100 height=50>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder**:  Go to the botttom of the notebook and shift-enter the [helper functions](#helpers)\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The success of a machine learning algorithm on a predictive problem is highly dependent on the way you as the practitioner present the data. Consider the following example. \n",
    "\n",
    "### Problem 1: Intuition\n",
    "***\n",
    "Suppose that you want to train a model to predict whether it is possible to drive between two cities in a single day.  The raw data includes the latitude and longitude of the two cities and the training data is labeled with $\\texttt{Yes}$ for is drivable and $\\texttt{No}$ for is not drivable.  One particular training set my look like \n",
    "\n",
    "|$\\texttt{CITY 1 LAT.}$ | $\\texttt{CITY 1 LNG.}$ |$\\texttt{CITY 2 LAT.}$ | $\\texttt{CITY 2 LNG.}$ | $\\texttt{DRIVABLE}$? | \n",
    "|:----:|:----:|:----:|:----:|:----:|\n",
    "| 123.24 |46.71\t| 121.33| 47.34\t| Yes |\n",
    "|123.24\t|56.91\t|121.33\t|55.23\t|Yes |\n",
    "|123.24\t|46.71\t|121.33\t|55.34\t|No |\n",
    "|123.24\t|46.71\t|130.99\t|47.34\t|No |\n",
    "\n",
    "**Q**: Given the following features, do you expect a linear classifier like Logistic Regression to be successful? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What features could you create that *would* be correlated with the correct classification? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Could we come up with a process to do this automatically?  Next question:  should we come up with a process to do this automatically? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Problem 2: Transformations on Continuous Data \n",
    "***\n",
    "\n",
    "Consider the case when you're trying to model quality of life of a person from survey data which asks a multitude of things like $\\texttt{income}$, $\\texttt{education level}$, $\\texttt{num children}$, etc.  As part of data exploration, you might try plotting the distributions of the raw data by feature individually.  A histogram of a potential data set for income might look as follows:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtkAAAGNCAYAAADEum3iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a4d7c76a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_inc = income_data()\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "n, bins, patches = ax.hist(x_inc, 15, normed=1, facecolor=mycolors[\"red\"], alpha=0.75, edgecolor=\"white\")\n",
    "ax.set_xlabel(\"income in tens of thousands\", fontsize=16)\n",
    "ax.set_ylabel(\"frequency\", fontsize=16);\n",
    "ax.set_title(\"Income Frequency\", fontsize=20);\n",
    "ax.grid(alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like we would expect for income.  There are lots of people at the low end of the spectrum, a few more in the middle, and very few in the high income categories.  \n",
    "\n",
    "There are two things to consider here.  First, lots of ML models assume that your input features are generally normally distributed.  Second, from a feature engineering perspective, we can think about how much income actually affects happiness.  It seems reasonable to believe that once you get to a certain level, increasing income has a diminishing effect on happiness.  \n",
    "\n",
    "Both of these viewpoints motivate us to try a log transformation on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAt8AAAGNCAYAAADJpB2lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxqvv/thwGgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a4d78cb0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_inc_log = np.log(x_inc)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "n, bins, patches = ax.hist(x_inc_log, 15, normed=1, facecolor=mycolors[\"blue\"], alpha=0.75, edgecolor=\"white\")\n",
    "ax.set_xlabel(\"log(income in thousands)\", fontsize=16)\n",
    "ax.set_ylabel(\"frequency\", fontsize=16);\n",
    "ax.set_title(\"Income Frequency\", fontsize=20);\n",
    "ax.grid(alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a distribution that looks fairly Gaussian, which might perform better in an machine learning model. \n",
    "\n",
    "<br>\n",
    "\n",
    "### Problem 3: Categorical Data \n",
    "*** \n",
    "\n",
    "Encoding categorical data can be tricky.  Consider the case when you have a raw feature in your model that corresponds to a person's hair color.  Possible values might be $\\texttt{blonde}$, $\\texttt{brunette}$, $\\texttt{redhead}$.  How should we encode these as numerical models in a ML algorithm? \n",
    "\n",
    "A natural (but misleading) thing to do would be to assign an integer to each possible value of the feature.  For instance, we could do \n",
    "\n",
    "|Religion| Feature|\n",
    "|:-------:|:------:|\n",
    "|blonde| 0 |\n",
    "|brunette | 1 | \n",
    "|redhead | 2|\n",
    "\n",
    "**Q**:  What is potentially wrong with this encoding, particularly in a regression context? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to fix this is create binary features corresponding to each religion.  That is, we create a binary $\\texttt{IsBlonde}$ variable, a binary $\\texttt{IsBrunette}$ variable, etc.  We then have \n",
    "\n",
    "|Religion| $\\texttt{IsBlonde}$ | $\\texttt{IsBrunette}$ | $\\texttt{IsRedhead}$| \n",
    "|:-------:|:------:|:------:|:------:|\n",
    "|Blonde| 1 | 0 | 0 | \n",
    "|Brunette | 0 | 1 | 0 | \n",
    "| Redhead | 0| 0 | 1 \n",
    "\n",
    "This process is called *one-hot-encoding* and is very frequently used to encode categorical data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "\n",
    "### Problem 4: Temporal Data \n",
    "*** \n",
    "\n",
    "Suppose that you're trying to train a model to predict that amount of foot-traffic at the 29th Street Mall.  Mall managers might be interested in such a model to predict the amount of janitorial and security services they need to employ at different times.  \n",
    "\n",
    "Suppose your training data consists of measurements of the amount of foot traffic at the mall and the date/time stamps that they were measured.  For instance, a training set might look like \n",
    "\n",
    "|$\\texttt{date_time_stamp}$| $\\texttt{FootTraffic}$|\n",
    "|:-------:|:------:|\n",
    "|$\\texttt{2015-11-12-20:00}$| 70|\n",
    "|$\\texttt{2015-06-10-21:00}$| 100|\n",
    "|$\\texttt{2015-08-02-12:00}$| 120|\n",
    "|$\\texttt{2015-12-22-12:00}$| 20|\n",
    "\n",
    "\n",
    "**Q**: How might you create meaningful features on the $\\texttt{date_time_stamp}$ data that would be useful for prediction? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 5: Part-of-Speech-Tagging\n",
    "*This problem and associated code was adapted from Jordan Boyd-Graber*\n",
    "***\n",
    "\n",
    "In computational linguistics, part-of-speech tagging (POST) is the process of marking a word in a text as a particular part of speech (e.g. noun, verb, adjective, etc), based on both its definition and its context. \n",
    "\n",
    "In this problem we will work with the <a href=\"https://en.wikipedia.org/wiki/Brown_Corpus\">Brown Corpus</a>, a compilation of 500 samples of English-language text totaling over a million words. The Brown Corpus is available through python's <a href=\"http://www.nltk.org/\">Natural Language Toolkit</a>.  Each word in the corpus has been tagged as: \n",
    "\n",
    "|type|symbol|\n",
    "|:--:|:----:|\n",
    "|adjective| JJ|\n",
    "|noun|NN|\n",
    "|pronoun|PP|\n",
    "|adverb|RB|\n",
    "|verb|VB|\n",
    "\n",
    "For the classification we will use simple Logistic Regression and focus on making iterative improvements by adding good features to our model.  The code for this problem is located in the helper functions section below.  Scroll down now and take a look at the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a Baseline**: When starting to evaluate the usefulness of features, it is usually a good idea to create a baseline.  That is, run your model with little to no features and see how the model performs.  The following code will run logistic regression with only a bias feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Features:\n",
      "\n",
      "\n",
      "\n",
      "Training Set\n",
      "-------------------------\n",
      "Accuracy: 0.526002\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "0\t522\t0\t0\t0\n",
      "0\t2205\t0\t0\t0\n",
      "0\t278\t0\t0\t0\n",
      "0\t179\t0\t0\t0\n",
      "0\t1008\t0\t0\t0\n",
      "\n",
      "Some misclassified examples:\n",
      "\n",
      "word: said        prediced pos: NN      true pos: VB\n",
      "word: he          prediced pos: NN      true pos: PP\n",
      "word: it          prediced pos: NN      true pos: PP\n",
      "word: his         prediced pos: NN      true pos: PP\n",
      "word: He          prediced pos: NN      true pos: PP\n",
      "word: its         prediced pos: NN      true pos: PP\n",
      "word: It          prediced pos: NN      true pos: PP\n",
      "word: federal     prediced pos: NN      true pos: JJ\n",
      "word: new         prediced pos: NN      true pos: JJ\n",
      "word: medical     prediced pos: NN      true pos: JJ\n",
      "\n",
      "Validation Set\n",
      "--------------------\n",
      "Accuracy: 0.560999\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "0\t129\t0\t0\t0\n",
      "0\t584\t0\t0\t0\n",
      "0\t59\t0\t0\t0\n",
      "0\t35\t0\t0\t0\n",
      "0\t234\t0\t0\t0\n",
      "\n",
      "Some misclassified examples:\n",
      "\n",
      "word: said        prediced pos: NN      true pos: VB\n",
      "word: he          prediced pos: NN      true pos: PP\n",
      "word: He          prediced pos: NN      true pos: PP\n",
      "word: made        prediced pos: NN      true pos: VB\n",
      "word: it          prediced pos: NN      true pos: PP\n",
      "word: its         prediced pos: NN      true pos: PP\n",
      "word: his         prediced pos: NN      true pos: PP\n",
      "word: local       prediced pos: NN      true pos: JJ\n",
      "word: It          prediced pos: NN      true pos: PP\n",
      "word: their       prediced pos: NN      true pos: PP\n"
     ]
    }
   ],
   "source": [
    "part_of_speech(limit=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so now we know that simply making predictions based on the number of occurrences of each type of speech leads to a *training accuracy* of around 52% and a *validation accuracy* of around 56%.  Hopefully we can improve on this by actually giving the model some useful features.  \n",
    "\n",
    "The obvious choice is to actually tell the model what the words is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Features:\n",
      "\n",
      "County\n",
      "Grand\n",
      "Jury\n",
      "said\n",
      "investigation\n",
      "recent\n",
      "primary\n",
      "election\n",
      "produced\n",
      "evidence\n",
      "irregularities\n",
      "took\n",
      "place\n",
      "\n",
      "\n",
      "Training Set\n",
      "-------------------------\n",
      "Accuracy: 0.962309\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "515\t6\t0\t0\t1\n",
      "12\t2186\t1\t1\t5\n",
      "0\t9\t269\t0\t0\n",
      "6\t75\t0\t98\t0\n",
      "2\t38\t0\t2\t966\n",
      "\n",
      "Some misclassified examples:\n",
      "\n",
      "word: increase    prediced pos: NN      true pos: VB\n",
      "word: public      prediced pos: JJ      true pos: NN\n",
      "word: further     prediced pos: JJ      true pos: RB\n",
      "word: Executive   prediced pos: NN      true pos: JJ\n",
      "word: place       prediced pos: NN      true pos: VB\n",
      "word: work        prediced pos: NN      true pos: VB\n",
      "word: issue       prediced pos: NN      true pos: VB\n",
      "word: back        prediced pos: RB      true pos: VB\n",
      "word: pay         prediced pos: VB      true pos: NN\n",
      "word: report      prediced pos: NN      true pos: VB\n",
      "\n",
      "Validation Set\n",
      "--------------------\n",
      "Accuracy: 0.792507\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "63\t65\t0\t1\t0\n",
      "4\t574\t0\t0\t6\n",
      "0\t6\t53\t0\t0\n",
      "2\t19\t0\t14\t0\n",
      "0\t113\t0\t0\t121\n",
      "\n",
      "Some misclassified examples:\n",
      "\n",
      "word: lacking     prediced pos: NN      true pos: VB\n",
      "word: agreed      prediced pos: NN      true pos: VB\n",
      "word: getting     prediced pos: NN      true pos: VB\n",
      "word: future      prediced pos: NN      true pos: JJ\n",
      "word: certain     prediced pos: NN      true pos: JJ\n",
      "word: you         prediced pos: NN      true pos: PP\n",
      "word: permitting  prediced pos: NN      true pos: VB\n",
      "word: going       prediced pos: NN      true pos: VB\n",
      "word: North       prediced pos: NN      true pos: JJ\n",
      "word: past        prediced pos: NN      true pos: JJ\n"
     ]
    }
   ],
   "source": [
    "part_of_speech(limit=500, word=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that seems more reasonable.  Now you should have a training accuracy of around 96% and a validation accuracy of around 79%.  It looks like using the words alone as features induces some overfitting on the training set. Let's see if we can think of new features that we could include that might help.  \n",
    "\n",
    "Here is where you have to use your brain to do feature engineering!  \n",
    "\n",
    "The code has output some useful information that you can use to do error analysis and hopefully come up with some useful features.  \n",
    "\n",
    "The top of the output gives you examples of the features that were used for several examples.  Since we only used the words as features, each example only includes the word itself. \n",
    "\n",
    "The next useful piece of information is shown in the *confusion matrix*. Sci-Kit Learn's confusion matrix function returns a matric $C$ such that $C_{ij}$ gives the number of examples known to be in group $i$ that were labeled as group $j$.  From the confusion matrix in the output we see that, in particular, the model is classifying a lot of words that should be verbs as nouns.   \n",
    "\n",
    "Now, based on this knowledge you might have some ideas about new features you can include to combat this error, but maybe you don't?  It's almost always a good idea to dig into the actual data and look at *specific* examples that your model has misclassified.  To help you with this, the code has printed some of the most common misclassifications.  \n",
    "\n",
    "**Q**: Looking at the common misclassifications, can you think of a good new feature to add? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your new feature to the model and see how it does! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Features:\n",
      "\n",
      "D:NOUN D:NOUN C:~C C:Co C:ou C:un C:nt C:ty C:y^ C:~Co C:Cou C:oun C:unt C:nty C:ty^ C:~Cou C:Coun C:ount C:unty C:nty^\n",
      "D:ADJ D:ADJ D:ADJ D:ADJ D:ADJ D:ADJ D:ADJ D:ADJ D:NOUN D:NOUN C:~G C:Gr C:ra C:an C:nd C:d^ C:~Gr C:Gra C:ran C:and C:nd^ C:~Gra C:Gran C:rand C:and^\n",
      "D:NOUN D:NOUN C:~J C:Ju C:ur C:ry C:y^ C:~Ju C:Jur C:ury C:ry^ C:~Jur C:Jury C:ury^\n",
      "D:ADJ D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB C:~s C:sa C:ai C:id C:d^ C:~sa C:sai C:aid C:id^ C:~sai C:said C:aid^\n",
      "D:NOUN D:NOUN C:~i C:in C:nv C:ve C:es C:st C:ti C:ig C:ga C:at C:ti C:io C:on C:n^ C:~in C:inv C:nve C:ves C:est C:sti C:tig C:iga C:gat C:ati C:tio C:ion C:on^ C:~inv C:inve C:nves C:vest C:esti C:stig C:tiga C:igat C:gati C:atio C:tion C:ion^\n",
      "D:ADJ D:ADJ D:NOUN C:~r C:re C:ec C:ce C:en C:nt C:t^ C:~re C:rec C:ece C:cen C:ent C:nt^ C:~rec C:rece C:ecen C:cent C:ent^\n",
      "D:ADJ D:ADJ D:ADJ D:ADJ D:ADJ D:NOUN D:NOUN D:NOUN D:NOUN C:~p C:pr C:ri C:im C:ma C:ar C:ry C:y^ C:~pr C:pri C:rim C:ima C:mar C:ary C:ry^ C:~pri C:prim C:rima C:imar C:mary C:ary^\n",
      "D:NOUN D:NOUN D:NOUN D:NOUN C:~e C:el C:le C:ec C:ct C:ti C:io C:on C:n^ C:~el C:ele C:lec C:ect C:cti C:tio C:ion C:on^ C:~ele C:elec C:lect C:ecti C:ctio C:tion C:ion^\n",
      "D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB C:~p C:pr C:ro C:od C:du C:uc C:ce C:ed C:d^ C:~pr C:pro C:rod C:odu C:duc C:uce C:ced C:ed^ C:~pro C:prod C:rodu C:oduc C:duce C:uced C:ced^\n",
      "D:VERB D:VERB D:VERB D:NOUN D:NOUN D:NOUN C:~e C:ev C:vi C:id C:de C:en C:nc C:ce C:e^ C:~ev C:evi C:vid C:ide C:den C:enc C:nce C:ce^ C:~evi C:evid C:vide C:iden C:denc C:ence C:nce^\n",
      "D:NOUN D:NOUN D:NOUN D:NOUN C:~i C:ir C:rr C:re C:eg C:gu C:ul C:la C:ar C:ri C:it C:ti C:ie C:es C:s^ C:~ir C:irr C:rre C:reg C:egu C:gul C:ula C:lar C:ari C:rit C:iti C:tie C:ies C:es^ C:~irr C:irre C:rreg C:regu C:egul C:gula C:ular C:lari C:arit C:riti C:itie C:ties C:ies^\n",
      "D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB C:~t C:to C:oo C:ok C:k^ C:~to C:too C:ook C:ok^ C:~too C:took C:ook^\n",
      "D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:VERB D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN D:NOUN C:~p C:pl C:la C:ac C:ce C:e^ C:~pl C:pla C:lac C:ace C:ce^ C:~pla C:plac C:lace C:ace^\n",
      "\n",
      "\n",
      "Training Set\n",
      "-------------------------\n",
      "Accuracy: 0.939408\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "468\t26\t0\t9\t19\n",
      "20\t2136\t0\t1\t48\n",
      "0\t13\t265\t0\t0\n",
      "12\t5\t0\t162\t0\n",
      "7\t94\t0\t0\t907\n",
      "\n",
      "Some misclassified examples:\n",
      "\n",
      "word: I           prediced pos: NN      true pos: PP\n",
      "word: present     prediced pos: VB      true pos: JJ\n",
      "word: meeting     prediced pos: VB      true pos: NN\n",
      "word: back        prediced pos: NN      true pos: RB\n",
      "word: increase    prediced pos: NN      true pos: VB\n",
      "word: East        prediced pos: NN      true pos: JJ\n",
      "word: public      prediced pos: JJ      true pos: NN\n",
      "word: force       prediced pos: NN      true pos: VB\n",
      "word: mention     prediced pos: NN      true pos: VB\n",
      "word: act         prediced pos: VB      true pos: NN\n",
      "\n",
      "Validation Set\n",
      "--------------------\n",
      "Accuracy: 0.927954\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "103\t14\t0\t3\t9\n",
      "7\t562\t0\t0\t15\n",
      "0\t2\t57\t0\t0\n",
      "1\t1\t0\t33\t0\n",
      "1\t22\t0\t0\t211\n",
      "\n",
      "Some misclassified examples:\n",
      "\n",
      "word: meeting     prediced pos: VB      true pos: NN\n",
      "word: present     prediced pos: VB      true pos: JJ\n",
      "word: work        prediced pos: VB      true pos: NN\n",
      "word: North       prediced pos: NN      true pos: JJ\n",
      "word: further     prediced pos: RB      true pos: JJ\n",
      "word: outmoded    prediced pos: VB      true pos: JJ\n",
      "word: fit         prediced pos: VB      true pos: JJ\n",
      "word: Tax         prediced pos: VB      true pos: NN\n",
      "word: top         prediced pos: NN      true pos: JJ\n",
      "word: official    prediced pos: JJ      true pos: NN\n"
     ]
    }
   ],
   "source": [
    "part_of_speech(limit=500,dictionary=True,characters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**:  Did your model do better on the test data?  Take a look at the confusion matrix and the examples of misclassifications and see if you can think of another new feature to add! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Features:\n",
      "\n",
      "\n",
      "\n",
      "Training Set\n",
      "-------------------------\n",
      "Accuracy: 0.526002\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "0\t522\t0\t0\t0\n",
      "0\t2205\t0\t0\t0\n",
      "0\t278\t0\t0\t0\n",
      "0\t179\t0\t0\t0\n",
      "0\t1008\t0\t0\t0\n",
      "\n",
      "Some misclassified examples:\n",
      "\n",
      "word: said        prediced pos: NN      true pos: VB\n",
      "word: he          prediced pos: NN      true pos: PP\n",
      "word: it          prediced pos: NN      true pos: PP\n",
      "word: his         prediced pos: NN      true pos: PP\n",
      "word: He          prediced pos: NN      true pos: PP\n",
      "word: its         prediced pos: NN      true pos: PP\n",
      "word: It          prediced pos: NN      true pos: PP\n",
      "word: federal     prediced pos: NN      true pos: JJ\n",
      "word: new         prediced pos: NN      true pos: JJ\n",
      "word: medical     prediced pos: NN      true pos: JJ\n",
      "\n",
      "Validation Set\n",
      "--------------------\n",
      "Accuracy: 0.560999\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "0\t129\t0\t0\t0\n",
      "0\t584\t0\t0\t0\n",
      "0\t59\t0\t0\t0\n",
      "0\t35\t0\t0\t0\n",
      "0\t234\t0\t0\t0\n",
      "\n",
      "Some misclassified examples:\n",
      "\n",
      "word: said        prediced pos: NN      true pos: VB\n",
      "word: he          prediced pos: NN      true pos: PP\n",
      "word: He          prediced pos: NN      true pos: PP\n",
      "word: made        prediced pos: NN      true pos: VB\n",
      "word: it          prediced pos: NN      true pos: PP\n",
      "word: its         prediced pos: NN      true pos: PP\n",
      "word: his         prediced pos: NN      true pos: PP\n",
      "word: local       prediced pos: NN      true pos: JJ\n",
      "word: It          prediced pos: NN      true pos: PP\n",
      "word: their       prediced pos: NN      true pos: PP\n"
     ]
    }
   ],
   "source": [
    "part_of_speech(limit=500, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**:  What's your performance like now?  Repeat this iterative process of adding new features until you're happy with your model (or you've exhausted the number of features available in the code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Features:\n",
      "\n",
      "\n",
      "\n",
      "Training Set\n",
      "-------------------------\n",
      "Accuracy: 0.526002\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "0\t522\t0\t0\t0\n",
      "0\t2205\t0\t0\t0\n",
      "0\t278\t0\t0\t0\n",
      "0\t179\t0\t0\t0\n",
      "0\t1008\t0\t0\t0\n",
      "\n",
      "Some misclassified examples:\n",
      "\n",
      "word: said        prediced pos: NN      true pos: VB\n",
      "word: he          prediced pos: NN      true pos: PP\n",
      "word: it          prediced pos: NN      true pos: PP\n",
      "word: his         prediced pos: NN      true pos: PP\n",
      "word: He          prediced pos: NN      true pos: PP\n",
      "word: its         prediced pos: NN      true pos: PP\n",
      "word: It          prediced pos: NN      true pos: PP\n",
      "word: federal     prediced pos: NN      true pos: JJ\n",
      "word: new         prediced pos: NN      true pos: JJ\n",
      "word: medical     prediced pos: NN      true pos: JJ\n",
      "\n",
      "Validation Set\n",
      "--------------------\n",
      "Accuracy: 0.560999\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "JJ\tNN\tPP\tRB\tVB\n",
      "0\t129\t0\t0\t0\n",
      "0\t584\t0\t0\t0\n",
      "0\t59\t0\t0\t0\n",
      "0\t35\t0\t0\t0\n",
      "0\t234\t0\t0\t0\n",
      "\n",
      "Some misclassified examples:\n",
      "\n",
      "word: said        prediced pos: NN      true pos: VB\n",
      "word: he          prediced pos: NN      true pos: PP\n",
      "word: He          prediced pos: NN      true pos: PP\n",
      "word: made        prediced pos: NN      true pos: VB\n",
      "word: it          prediced pos: NN      true pos: PP\n",
      "word: its         prediced pos: NN      true pos: PP\n",
      "word: his         prediced pos: NN      true pos: PP\n",
      "word: local       prediced pos: NN      true pos: JJ\n",
      "word: It          prediced pos: NN      true pos: PP\n",
      "word: their       prediced pos: NN      true pos: PP\n"
     ]
    }
   ],
   "source": [
    "part_of_speech(limit=500, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 6: Building Multiple Features in SKLearn with Custom Transformers and FeatureUnion\n",
    "***\n",
    "\n",
    "In the Feature Engineering assignment your goal is to hand-craft features to predict whether statements about TV shows contain spoilers or not.  There are endless possibilities for useful features that you might want to try out, but for those of you not familiar with text-learning in sklearn just the process of getting up and running might seem daunting.  In this problem I will demonstrate the use of two particular objects that may make your life significantly easier: the generic Transformer and the FeatureUnion pipeline.  We will also make use of the CountVectorizer which is similar to the HashVectorizer seen in Problem 5.  If you have significant experience in text-learning then you have likely seen these things before (and/or know better things) and can safely skip this exercise. \n",
    "\n",
    "For the purpose of this discussion we will assume, like in the homework, that our data is a list of sentence strings.  For instance, we might have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = [\n",
    "    \"The quick brown fox jumped over the lazy lazy dog\",\n",
    "    \"There is that dog and fox again\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to turn these data into a vectorized bag-of-word representation we can use an instance of CountVectorizer, which is a simple transformer that turns raw text into bag-or-words vectors.  To make the number of word-features more manageable I will call CountVectorizer with a list of stop words to be removed.  \n",
    "\n",
    "To transform the data into a matrix I simply call the $\\texttt{fit_transform}$ method on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_words = CountVectorizer(stop_words='english')\n",
    "X = bag_of_words.fit_transform(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I want to see the names of the specific features created (and important for our purpose, the order in which they're encoded) I can call the $\\texttt{get_feature_names}$ method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The named features are  ['brown', 'dog', 'fox', 'jumped', 'lazy', 'quick']\n"
     ]
    }
   ],
   "source": [
    "print(\"The named features are \", bag_of_words.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that CountVectorizer removed most of the common words like \"the\", \"there\", \"is\" and \"and\" and stores the important words in alphabetical order. \n",
    "\n",
    "Let's check the type and shape of the matrix produced by the transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X has type  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "X has shape  (2, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"X has type \", type(X))\n",
    "print(\"X has shape \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So CountVectorizer returns our vectorized features as a sparse matrix.  This makes sense since most feature vectors in text applications only contain a handful of vocabulary words and are therefore very sparse. \n",
    "\n",
    "Finally, the matrix X has two rows and six columns.  Each row corresponds to a training example (a sentence) and each column to a word-feature with the order corresponding to the list returned by $\\texttt{get_feature_names}$. \n",
    "\n",
    "Let's look at the matrix and see if it does what we think it's doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 2 1]\n",
      " [0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row of X should refer to the first sentence in $\\texttt{train}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumped over the lazy lazy dog\n"
     ]
    }
   ],
   "source": [
    "print(train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first column refers to instances of \"brown\", and the fifth column refers to instances of \"lazy\".  OK, so there's an example of a simple canned transformer that we can apply to raw text-data.  But what if we want to craft something a little less standard?  What if, for instance, we're convinced that the number of times the characters \"x\", \"y\", and \"z\" appear in a sentence is somehow an important feature (probably not, but just go with it).  How could add these features to our data matrix?   \n",
    "\n",
    "There are definitely simple hacky ways to do this, but one slick way is to write your own custom transformer.  This transformer will take in raw text-data, turn them into numeric feature vectors (counts of the number of \"x\", \"y\", and \"z\"s) and return the matrix of transformed data.  One such transformer might look as follows  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class XYZTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        import numpy as np \n",
    "        from scipy.sparse import csr_matrix\n",
    "        \n",
    "        letters = ['x', 'y', 'z']\n",
    "         \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples), len(letters)))\n",
    "        \n",
    "        # Loop over examples and count letters \n",
    "        for ii, x in enumerate(examples):\n",
    "            X[ii,:] = np.array([x.count(letter) for letter in letters])\n",
    "            \n",
    "        return csr_matrix(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially we're writing a class that takes in the type of data we expect, transforms it the way we expect, and returns it the way we expect.  One thing to note is that by convention the transformer contains a $\\texttt{fit}$ method that only results $\\texttt{self}$.  All of the magic actually happens in the $\\texttt{transform}$ method. \n",
    "\n",
    "Let's test it out on our simple training data and see if it does what we expect.  Remember that the sentences in the training data are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumped over the lazy lazy dog\n",
      "There is that dog and fox again\n"
     ]
    }
   ],
   "source": [
    "print(train[0])\n",
    "print(train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  2.]\n",
      " [ 1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the XYZTransformer \n",
    "xyz = XYZTransformer()\n",
    "\n",
    "# Fit it to our data \n",
    "Y = xyz.fit_transform(train)\n",
    "\n",
    "# Print a dense version of the matrix \n",
    "print(Y.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first row (corresponding to the first sentence in the data) indicates that there is 1 x, 2 y's, and 2 z's in the first sentence, which you can verify is correct.  Similarly, the second row indicates that there is a single x (from \"fox\") in the sentence and no y's or z's. \n",
    "\n",
    "OK, so you've built your first transformer.  Now, let's say we want to combine the bag-of-words vectors and the letter features into a single data matrix.  We can do this with a particular class called a $\\texttt{FeatureUnion}$.  If you're familiar with sklearn's pipelines, know that a $\\texttt{FeatureUnion}$ is a pipeline specifically designed for transformer objects. \n",
    "\n",
    "Now we'll combine our two transformers into a mega-transformer that will zip all of our features into a nice package.  It might look as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "allmyfeatures = FeatureUnion([\n",
    "        (\"bag-of-words\", CountVectorizer(stop_words='english')),\n",
    "        (\"letter-counts\", XYZTransformer())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll call our mega-transformer on the original data, and hopefully get a sparse matrix out that encapsulates all of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z has type  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Z has shape  (2, 9)\n"
     ]
    }
   ],
   "source": [
    "Z = allmyfeatures.fit_transform(train)\n",
    "print(\"Z has type \", type(Z))\n",
    "print(\"Z has shape \", Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the combined data matrix Z is again a csr matrix (by design) and has two rows and nine columns.  The two rows again correspond to the two pieces of data in the train set, and the nine columns correspond to the six bag-of-word features from CountVectorizer and the three letter count features from XYZTransformer.  If we print a dense version of Z ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.  2.  1.  1.  2.  2.]\n",
      " [ 0.  1.  1.  0.  0.  0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Z.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you will see that it looks exactly like the data matrices X and Y concatenated side-by-side. \n",
    "\n",
    "This is certainly not the only way to combine features from different transformers, but in my experience it is definitely one of the cleanest, especially when working with many types of hand-crafted features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "\n",
    "<a id='helpers'></a>\n",
    "\n",
    "<br> \n",
    "\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\prash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import string\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('brown')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import brown\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import seaborn as sn \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\"}\n",
    "\n",
    "\n",
    "def normalize_tags(tag):\n",
    "    if not tag or not tag[0] in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
    "        return \"PUNC\"\n",
    "    else:\n",
    "        return tag[:2]\n",
    "\n",
    "\n",
    "kTAGSET = [\"\", \"JJ\", \"NN\", \"PP\", \"RB\", \"VB\"]\n",
    "\n",
    "class Analyzer:\n",
    "    def __init__(self, word, before, after, prev, next, char, dict):\n",
    "        self.word = word\n",
    "        self.after = after\n",
    "        self.before = before\n",
    "        self.prev = prev\n",
    "        self.next = next\n",
    "        self.dict = dict\n",
    "        self.char = char\n",
    "\n",
    "    def __call__(self, feature_string):\n",
    "        feats = feature_string.split()\n",
    "\n",
    "        if self.word:\n",
    "            yield feats[0]\n",
    "\n",
    "        if self.after:\n",
    "            for ii in [x for x in feats if x.startswith(\"A:\")]:\n",
    "                yield ii\n",
    "        if self.before:\n",
    "            for ii in [x for x in feats if x.startswith(\"B:\")]:\n",
    "                yield ii\n",
    "        if self.prev:\n",
    "            for ii in [x for x in feats if x.startswith(\"P:\")]:\n",
    "                yield ii\n",
    "        if self.next:\n",
    "            for ii in [x for x in feats if x.startswith(\"N:\")]:\n",
    "                yield ii\n",
    "        if self.dict:\n",
    "            for ii in [x for x in feats if x.startswith(\"D:\")]:\n",
    "                yield ii\n",
    "        if self.char:\n",
    "            for ii in [x for x in feats if x.startswith(\"C:\")]:\n",
    "                yield ii\n",
    "                \n",
    "def example(sentence, position):\n",
    "        word = sentence[position][0]\n",
    "        ex = word\n",
    "        tag = normalize_tags(sentence[position][1])\n",
    "        if tag in kTAGSET:\n",
    "            target = kTAGSET.index(tag)\n",
    "        else:\n",
    "            target = None\n",
    "\n",
    "        if position > 0:\n",
    "            prev = \" P:%s\" % sentence[position - 1][0]\n",
    "        else:\n",
    "            prev = \"\"\n",
    "\n",
    "        if position < len(sentence) - 1:\n",
    "            next = \" N:%s\" % sentence[position + 1][0]\n",
    "        else:\n",
    "            next = ''\n",
    "\n",
    "        all_before = \" \" + \" \".join([\"B:%s\" % x[0] for x in sentence[:position]])\n",
    "        all_after = \" \" + \" \".join([\"A:%s\" % x[0] for x in sentence[(position + 1):]])\n",
    "\n",
    "        dictionary = [\"D:ADJ\"] * len(wn.synsets(word, wn.ADJ)) + \\\n",
    "          [\"D:ADV\"] * len(wn.synsets(word, wn.ADV)) + \\\n",
    "          [\"D:VERB\"] * len(wn.synsets(word, wn.VERB)) + \\\n",
    "          [\"D:NOUN\"] * len(wn.synsets(word, wn.NOUN))\n",
    "\n",
    "        dictionary = \" \" + \" \".join(dictionary)\n",
    "\n",
    "        char = ' '\n",
    "        padded_word = \"~%s^\" % sentence[position][0]\n",
    "        for ngram_length in range(2, 5):\n",
    "            char += ' ' + \" \".join(\"C:%s\" % \"\".join(cc for cc in x)\n",
    "                                   for x in ngrams(padded_word, ngram_length))\n",
    "        ex += char\n",
    "\n",
    "        ex += prev\n",
    "        ex += next\n",
    "        ex += all_after\n",
    "        ex += all_before\n",
    "        ex += dictionary\n",
    "\n",
    "        return ex, target\n",
    "    \n",
    "def all_examples(limit, train=True):\n",
    "    sent_num = 0\n",
    "    for ii in brown.tagged_sents():\n",
    "        sent_num += 1\n",
    "        if limit > 0 and sent_num > limit:\n",
    "            break\n",
    "\n",
    "        for jj in range(len(ii)):\n",
    "            ex, tgt = example(ii, jj)\n",
    "            if tgt:\n",
    "                if train and sent_num % 5 != 0:\n",
    "                    yield ex, tgt\n",
    "                if not train and sent_num % 5 == 0:\n",
    "                    yield ex, tgt\n",
    "                    \n",
    "def accuracy(classifier, x, y, examples):\n",
    "    predictions = classifier.predict(x)\n",
    "    cm = confusion_matrix(y, predictions)\n",
    "\n",
    "    print(\"Accuracy: %f\" % accuracy_score(y, predictions))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\\n\")\n",
    "    print(\"\\t\".join(kTAGSET[1:]))\n",
    "    for ii in cm:\n",
    "        print(\"\\t\".join(str(x) for x in ii))\n",
    "\n",
    "    errors = defaultdict(int)\n",
    "    for ii, ex_tuple in enumerate(examples):\n",
    "        ex, tgt = ex_tuple\n",
    "        if tgt != predictions[ii]:\n",
    "            errors[(ex.split()[0], kTAGSET[predictions[ii]], kTAGSET[tgt])] += 1\n",
    "\n",
    "    print(\"\\nSome misclassified examples:\\n\")\n",
    "    for ww, cc in sorted(errors.items(), key=operator.itemgetter(1), reverse=True)[:10]:\n",
    "        print(\"word: {:10s}  prediced pos: {:2s}      true pos: {:2s}\".format(ww[0], ww[1], ww[2]))\n",
    "        \n",
    "def part_of_speech(**kwargs):\n",
    "    word = kwargs.get(\"word\", False)\n",
    "    all_before = kwargs.get(\"all_before\", False)\n",
    "    all_after = kwargs.get(\"all_after\", False)\n",
    "    one_before = kwargs.get(\"one_before\", False)\n",
    "    one_after = kwargs.get(\"one_after\", False)\n",
    "    characters = kwargs.get(\"characters\", False)\n",
    "    dictionary = kwargs.get(\"dictionary\", False)\n",
    "    limit= kwargs.get(\"limit\",-1)\n",
    "    \n",
    "    analyzer = Analyzer(word, all_before, all_after, one_before, one_after, characters, dictionary)\n",
    "    \n",
    "    vectorizer = HashingVectorizer(analyzer=analyzer)\n",
    "\n",
    "    x_train = vectorizer.fit_transform(ex for ex, tgt in all_examples(limit))\n",
    "    x_valid = vectorizer.fit_transform(ex for ex, tgt in all_examples(limit, train=False))\n",
    "    \n",
    "    print(\"Example Features:\")\n",
    "    exstr = \"\\n\"\n",
    "    for ex, tgt in all_examples(1):\n",
    "        exstr += \" \".join(analyzer(ex)) + \"\\n\"\n",
    "    if exstr.replace(\" \", \"\").replace(\"\\n\", \"\") == \"\":\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(exstr)\n",
    "\n",
    "    y_train = np.array(list(tgt for ex, tgt in all_examples(limit)))\n",
    "    y_valid = np.array(list(tgt for ex, tgt in all_examples(limit, train=False)))\n",
    "\n",
    "    lr = SGDClassifier(loss='log', penalty='l2', tol=None, max_iter=5, shuffle=True)\n",
    "    lr.fit(x_train, y_train)\n",
    "\n",
    "    print(\"\\nTraining Set\\n-------------------------\")\n",
    "    accuracy(lr, x_train, y_train, all_examples(limit))\n",
    "    print(\"\\nValidation Set\\n--------------------\")\n",
    "    accuracy(lr, x_valid, y_valid, all_examples(limit, train=False))\n",
    "    \n",
    "np.random.seed(1234)\n",
    "    \n",
    "def income_data(N=200):\n",
    "    x = 1.1*np.random.normal(size=N)\n",
    "    y = np.exp(x)\n",
    "    return y\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5: SGD Variants and Multiclass Classification \n",
    "***\n",
    "\n",
    "<img src=\"figs/mountains.jpg\" width=1100  height=50>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder**:  Scroll down and shift-enter the **Helper Functions**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: The Learning Rate Schedule Game\n",
    "***\n",
    "\n",
    "In the case when your negative log-likelihood function is convex, the choice of learning rate mainly affects the convergence of your SGD routine.  In a nonconvex problem, the choice of learning rate can determine whether you find the global minimum, or get stuck forever in a local minimum.  In most sophisticated optimization routines, the learning rate is adapted over time.  Varying learning rate schedules allow you to explore local minimums but still be able to make it out and eventually find the global minimum.  \n",
    "\n",
    "The following game is a cheap facsimile of stochastic gradient descent.  There is no negative log-likelihood function, or training set.  You just have a simple function that you would like to minimize, namely \n",
    "\n",
    "$$\n",
    "f(x,y) = 5-\\sin(3 \\pi x) ~ \\sin(3 \\pi y) - 3~\\textrm{exp}\\left[{-\\left(x-\\frac{1}{2}\\right)^2 - \\left(y-\\frac{1}{2}\\right)^2}\\right]\n",
    "$$\n",
    "\n",
    "The surface looks as follows.  Notice that there is a global minimum at $(1/2,1/2)$ and several local maxima and minima surrounding it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nc_surface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've given you a starting point and a basic gradient descent algorithm (located in the Helper Functions section below).  Below this cell there is a learning rate scheduling function that currently just returns the initial learning rate that you prescribe.  The goal of this game is for you to adjust the initial learning rate and the scheduling function that allows the iterate to make it to the global minimum.  The **only** things you're allowed to change are the **initial learning rate** and the **schedule function**.   Before you can play you need to evaluate the code-blocks at the bottom of the page.  Then come back and evaluate the $\\texttt{playgame}$ function with it's current inputs and see what happens! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule(k, n, eta0):\n",
    "    '''\n",
    "    :param k: The current iteration \n",
    "    :param n: The max number of iterations\n",
    "    :param eta0: The original learning rate \n",
    "    '''\n",
    "    return eta0\n",
    "\n",
    "playgame(np.array([0.15,0.0]), 150, .01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Hint**: If you're not having much luck, try implementing a schedule of the form \n",
    "$\n",
    "\\eta_k = \\dfrac{\\eta_0}{ 1 + \\alpha ~ k~/~n}\n",
    "$ where here $\\alpha$ is a tuning parameter.  You'll probably also have to make your initial learning rate bigger. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Momentum \n",
    "***\n",
    "\n",
    "In this problem you can play around with a cheap facsimile for SGD with momentum.  This time we'll keep the learning rate constant and work on a deterministic problem with a convex objective function.  Namely, we'll try to find the minimum of \n",
    "\n",
    "$$\n",
    "f(x,y) = 1 + \\frac{1}{100}(x^2 + 20y^2)\n",
    "$$\n",
    "\n",
    "which has it's minimum of $1$ at the point $(0,0)$.  \n",
    "\n",
    "Execute the following function to see a plot of the level curves of $f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_con_surface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elongated level curves such as this can often cause problems for gradient descent algorithms.  The following function implements gradient descent with momentum, but can impersonate vanilla gradient sent by keeping the momentum parameter, $\\mu$, set to zero.  Play around with various values of the learning rate, $\\eta$, and see what happens. Once you've settled on a reasonable learning rate, play around with the momentum parameter and see if you can make improvements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playmomentum(np.array([-4.0,-1.0]), 50, eta0=5.0, mu=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "### Problem 3: Multiclass Issues \n",
    "***\n",
    "\n",
    "**Q**: What are some potential difficulties with the One-vs-All binary classification scheme? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What are some potential remedies for this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<a id='prob2ans'></a>\n",
    "\n",
    "### Problem 4: Multiclass Classification and Error Correcting Output Codes \n",
    "***\n",
    "\n",
    "We've seen the One-vs-All and All-Pairs framework for reducing multiclass classification to binary classification.  In this problem we explore a similar (but different) framework that has become popular based on the theory of error correcting codes.  Consider the following binary coding matrix: \n",
    "\n",
    "$$\n",
    "\\begin{array}{ccccc}\n",
    "\\hline\n",
    "\\textrm{Class} & h_0 & h_1 & h_2 & h_3 & h_4 \\\\ \n",
    "\\hline\n",
    "0 & 1 & 0 & 1 & 0 & 1 \\\\\n",
    "1 & 0 & 0 & 1 & 1 & 1 \\\\\n",
    "2 & 1 & 1 & 0 & 0 & 0 \\\\\n",
    "3 & 1 & 1 & 1 & 1 & 0 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Think of each column in the binary coding matrix as a hypothesis where the entries in the columns define binary *super classes*. The general idea is similar to One-vs-All and All-Pairs in the sense that we'll assign binary labels to different classes for each classifier, perform binary classifications, and then assign a final class to a query point according to some procedure.   The motivation behind the ECOC scheme is that the combinations of the bit representation of each class should be robust to minor classification errors.  \n",
    "\n",
    "To see this, notice that in the example above each bit-string or *codeword* associated with each class differs by at least **two** bits, i.e. their Hamming Distances are at least 2.  What this means is that if you make predictions based on the trained binary classifiers to obtain a codeword representation of a query point, even if it's off by one predicted bit you **might** be able to obtain the correct general classification by using a distance measure.  \n",
    "\n",
    "Here is an example of an ECOC scheme for classifying handwritten digits for the MNIST data set: \n",
    "\n",
    "<img src=\"figs/k15ecoc.png\" width=300 height=50>\n",
    "\n",
    "Any guesses what the hypotheses in the column headers mean?\n",
    "\n",
    "<p> </p>\n",
    "\n",
    "**Example**: Suppose you want to predict the nationality of a person given their last name.  Your training set is \n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\hline \n",
    "\\textrm{Korean} & \\textrm{German} & \\textrm{Argentine} \\\\\n",
    "\\hline \n",
    "\\textrm{Park} & \\textrm{Mann} & \\textrm{Puig} \\\\\n",
    "\\textrm{Kim} & \\textrm{Berndt} & \\textrm{Borges} \\\\\n",
    "\\textrm{Kwon} & \\textrm{Grass} & \\textrm{Cortazar} \\\\\n",
    "\\hline \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Suppose that the codebook you will use looks like \n",
    "\n",
    "$$\n",
    "\\begin{array}{ccccc}\n",
    "\\hline\n",
    "\\textrm{Class} & h_0 & h_1 & h_2 & h_3 \\\\ \n",
    "\\hline\n",
    "\\textrm{Korean}    & 1 & 0 & 0 & 1  \\\\\n",
    "\\textrm{German}    & 0 & 0 & 1 & 0  \\\\\n",
    "\\textrm{Argentine} & 1 & 1 & 1 & 0  \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**Q**: What are the binary labels corresponding to each training example under each hypothesis? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Suppose that the predictions for a given query point are given by $\\hat{y} = (0,0,0,1)$.  Which class would we predict? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Suppose that the predictions for a given query point are given by $\\hat{y} = (1,1,0,0)$.  Which class would we predict? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: For a classification problem with three classes, what would the codebook be corresponding to One-vs-All classification? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: How many useful codewords can you have for a three-class classifier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What is a general formula for the number of useful hypotheses in a K-class setting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on ECOCs in multiclass classification check out the original paper by <a href=\"https://www.jair.org/media/105/live-105-1426-jair.pdf\">Dietterich and Bakiri</a> or this paper by <a href=\"http://amstat.tandfonline.com/doi/abs/10.1080/10618600.1998.10474782\">James and Hastie</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "\n",
    "<a id='helpers'></a>\n",
    "\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def myncfun(x, y): \n",
    "    return 5 - np.sin(3*np.pi*x)*np.sin(3*np.pi*y) - 3*np.exp(-(x-.5)**2 - (y-.5)**2)\n",
    "\n",
    "def myncgrad(x):\n",
    "    g1 = -3 * np.pi * np.cos(3*np.pi*x[0]) * np.sin(3*np.pi*x[1]) + 6 * (x[0]-.5) * np.exp(-(x[0]-.5)**2 - (x[1]-.5)**2)\n",
    "    g2 = -3 * np.pi * np.sin(3*np.pi*x[0]) * np.cos(3*np.pi*x[1]) + 6 * (x[1]-.5) * np.exp(-(x[0]-.5)**2 - (x[1]-.5)**2)\n",
    "    return np.array([g1, g2])\n",
    "\n",
    "def SGD(x, numstep, eta0):\n",
    "    '''\n",
    "    :param x: Starting point \n",
    "    :param numstep: Total number iterations \n",
    "    :param eta0: Initial learning rate \n",
    "    '''\n",
    "    xhist = np.zeros((numstep+1,2))\n",
    "    xhist[0,:] = x \n",
    "    for kk in range(numstep):\n",
    "        x = x - schedule(kk, numstep, eta0) * myncgrad(x)\n",
    "        xhist[kk+1,:] = x \n",
    "    return xhist\n",
    "\n",
    "def playgame(x0, numstep, eta0):\n",
    "    '''\n",
    "    :param x0: The starting point \n",
    "    :param numstep: The total number of iterations to do \n",
    "    :param eta0: The original learning rate \n",
    "    '''\n",
    "    xx, yy = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 200))\n",
    "    Z = myncfun(xx, yy)\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    CS = plt.contour(xx, yy, Z)\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "\n",
    "    xhist = SGD(x0, numstep, eta0)\n",
    "    \n",
    "    fvals = np.zeros(numstep+1)\n",
    "    fvals[0] = myncfun(x0[0], x0[1])\n",
    "\n",
    "    for ii in range(xhist.shape[0]-1):\n",
    "        x0 = xhist[ii][0]\n",
    "        y0 = xhist[ii][1]\n",
    "        x1 = xhist[ii+1][0]\n",
    "        y1 = xhist[ii+1][1]\n",
    "        ax1.plot([x0, x1], [y0,y1], color=\"black\", marker=\"o\", lw=1.5, markersize=5)\n",
    "        fvals[ii+1] = myncfun(x0, y0)\n",
    "        \n",
    "    plt.xlabel(\"x1\", fontsize=16)\n",
    "    plt.ylabel(\"x2\", fontsize=16)\n",
    "        \n",
    "    maxval = myncfun(0.5,0.5)\n",
    "        \n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.plot(fvals, 'r--', marker=\"o\")\n",
    "    ax2.plot([0, numstep+1], [maxval, maxval], 'k--', lw=2, alpha=0.5)\n",
    "    plt.xlim([0,numstep+1])\n",
    "    plt.ylim([.5,3])\n",
    "    plt.xlabel(\"iteration\", fontsize=16)\n",
    "    plt.ylabel(\"function value\", fontsize=16);\n",
    "    \n",
    "    \n",
    "def plot_nc_surface():\n",
    "    xx, yy = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 200))\n",
    "    Z = myncfun(xx, yy)\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    CS = plt.contour(xx, yy, Z)\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlabel(\"x1\", fontsize=16)\n",
    "    plt.ylabel(\"x2\", fontsize=16)    \n",
    "    \n",
    "    \n",
    "def plot_con_surface():\n",
    "    xx, yy = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-2.5, 2.5, 200))\n",
    "    Z = myfun(xx, yy)\n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    CS = plt.contour(xx, yy, Z, 5)\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "    plt.xlim([-5,5])\n",
    "    plt.ylim([-2.5,2.5])\n",
    "    plt.xlabel(\"x1\", fontsize=16)\n",
    "    plt.ylabel(\"x2\", fontsize=16)   \n",
    "    \n",
    "def playmomentum(x0, numstep, eta0, mu):\n",
    "    '''\n",
    "    :param x0: The starting point \n",
    "    :param numstep: The total number of iterations to do \n",
    "    :param eta0: The original learning rate \n",
    "    '''\n",
    "    xx, yy = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-2.5, 2.5, 200))\n",
    "    Z = myfun(xx, yy)\n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    CS = plt.contour(xx, yy, Z, 5)\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "    plt.xlim([-5,5])\n",
    "    plt.ylim([-2.5,2.5])\n",
    "    plt.xlabel(\"x1\", fontsize=16)\n",
    "    plt.ylabel(\"x2\", fontsize=16)\n",
    "\n",
    "    xhist = momentumSGD(x0, numstep, eta0, mu)\n",
    "    \n",
    "    fvals = np.zeros(numstep+1)\n",
    "    fvals[0] = myfun(x0[0], x0[1])\n",
    "\n",
    "    for ii in range(xhist.shape[0]-1):\n",
    "        x0 = xhist[ii][0]\n",
    "        y0 = xhist[ii][1]\n",
    "        x1 = xhist[ii+1][0]\n",
    "        y1 = xhist[ii+1][1]\n",
    "        ax1.plot([x0, x1], [y0,y1], color=\"black\", marker=\"o\", lw=1.5, markersize=5)\n",
    "        fvals[ii+1] = myfun(x0, y0)\n",
    "        \n",
    "    plt.xlabel(\"x1\", fontsize=16)\n",
    "    plt.ylabel(\"x2\", fontsize=16)\n",
    "        \n",
    "    maxval = myfun(0.0,0.0)\n",
    "        \n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.plot(fvals, 'r--', marker=\"o\")\n",
    "    ax2.plot([0, numstep+1], [maxval, maxval], 'k--', lw=2, alpha=0.5)\n",
    "    plt.xlim([0,numstep+1])\n",
    "    plt.ylim([.5,1.5])\n",
    "    plt.xlabel(\"iteration\", fontsize=16)\n",
    "    plt.ylabel(\"function value\", fontsize=16);\n",
    "    \n",
    "def momentumSGD(x, numstep, eta0, mu):\n",
    "    '''\n",
    "    :param x: Starting point \n",
    "    :param numstep: Total number iterations \n",
    "    :param eta0: Initial learning rate \n",
    "    '''\n",
    "    xhist = np.zeros((numstep+1,2))\n",
    "    xhist[0,:] = x \n",
    "    vold = 0.0*x \n",
    "    for kk in range(numstep):\n",
    "        vnew = eta0 * mygrad(x) + mu * vold \n",
    "        vold = vnew \n",
    "        x = x - vnew \n",
    "        xhist[kk+1,:] = x \n",
    "    return xhist\n",
    "\n",
    "def mygrad(x):\n",
    "    g1 = .02*x[0]\n",
    "    g2 = 0.4*x[1]\n",
    "    return np.array([g1, g2])\n",
    "\n",
    "def myfun(x, y): \n",
    "    return 1.0 + 0.01*(x**2 + 20*y**2)\n",
    "    \n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

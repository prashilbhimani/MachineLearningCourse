{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8: Linear Regresion and the Bias-Variance Trade-Off\n",
    "***\n",
    "\n",
    "<img src=\"figs/targetsBannerTry.png\" width=1100 height=50>\n",
    "\n",
    "**Reminder**:  Go to the botttom of the notebook and shift-enter the helper functions.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Linear Regression on Dam Data \n",
    "***\n",
    "\n",
    "The data we will explore in this notebook relates the change in the water level in a damn to the rate of flow of water through the damn. First we'll load the data which is stored in a serialized format in the data directory.  We'll store the features in a 2D Numpy array $X$ and the response in a 1D Numpy array $y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/dam_regression.pickle\", \"rb\") as fname: dam_data = pickle.load(fname)\n",
    "X = dam_data[\"features\"]\n",
    "y = dam_data[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we explore the data a bit we'll see that there are 54 examples in the data set with a single feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 54 examples in the data set\n",
      "Each example contains 1 feature(s)\n"
     ]
    }
   ],
   "source": [
    "n_examples = len(y)\n",
    "n_features = X.shape[1]\n",
    "print(\"There are {:2d} examples in the data set\".format(n_examples))\n",
    "print(\"Each example contains {:d} feature(s)\".format(n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll look at the data by making a scatter plot.  The following function (found at the bottom of this notebook) will do all of the plotting for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfkAAAGNCAYAAAASBQgzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20be3bf5a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dam_plot([(X, y, \"data\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: How would you characterize relationship between the feature $X$ and the response $y$? Is the relationship linear?  Is it nonlinear?  Is it highly nonlinear? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we'll use Scikit-Learn to build a simple linear regression model for our data. As usual, the first step in our process will be to divide the data into a training and a validation set.  Scikit-learn has a handy function to do this for us called [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Clicking the link will take you to the documentation.  You'll see in the documentation that the function takes in the complete data matrix and response vector, as well as a specification of the fraction of data to go in the training set.  Finally, it takes a `random_state` value that sets the seed of the random number generator.  We'll specify a random state so that everyone's data looks the same.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=1734)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've selected a split with $80\\%$ and $20\\%$ of the data split into the training set and validation set, respectively.  Executing the following cell will plot the training and validation data together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dam_plot([(X_train, y_train, \"training\"), (X_valid, y_valid, \"validation\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll build a linear regression model to fit the data using Scikit-Learn's [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class. Use the link to check out the documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've only created an instance of the class.  Next we need to fit the model to our data.  Almost all models in Scikit-Learn come with a `.fit` method used to train the model.  We need to pass in the training features in `X_train` and the training responses in `y_train`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we've trained our linear regression with Scikit-Learn.  But how successful were we?  One way we might check is by looking at the MSE on the training data and on the validation data. In order to compute the MSE we first need to compute the predictions of the model on the training data and the validation data. We can do this using the `predict` method and passing in the training features and the validation features stored in `X_train` and `X_valid`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat_train = linreg.predict(X_train)\n",
    "yhat_valid = linreg.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the `mean_square_error` method to compute the MSE of the predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse_train = mean_squared_error(yhat_train, y_train)\n",
    "mse_valid = mean_squared_error(yhat_valid, y_valid)\n",
    "\n",
    "print(\"Training MSE:   {:.3f}\".format(mse_train))\n",
    "print(\"Validation MSE: {:.3f}\".format(mse_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: The MSE is a relatively simple calculation that potentially doesn't warrant a call to an sklearn function.  Can you verify these values by computing them directly in Numpy? Remember that the formula for MSE is given by \n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\hat{y}_i\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_mse_train = 0.0 #TODO \n",
    "my_mse_valid = 0.0 #TODO \n",
    "print(\"Training MSE:   {:.3f}\".format(my_mse_train))\n",
    "print(\"Validation MSE: {:.3f}\".format(my_mse_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Finally, let's plot our linear model against our training and validation data.  Like most plotting libraries, Matplotlib takes in an arrays of $x$-values and $y$-values and simply connects the dots.  In order to generate some plotting data, one thing we could do is create an array of equispaced $x$-values an then use our model prediction to get an array of $y$-values.  Fill in the missing code below to accomplish this. \n",
    "\n",
    "**Note**: The quirky `.reshape(-1,1)` call below turns the 1D array that `linespace` returns into a 2D array.  Pretty uniformly across Scikit-Learn classes, the `fit` and `predict` methods expect your features to be a 2D array.  Learn this quirk now to avoid many painful `DeprecationWarning`'s in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xplot = np.linspace(-60,60,100).reshape(-1,1)\n",
    "yplot = np.zeros_like(xplot) # TODO - Fix This  \n",
    "dam_plot([(X_train, y_train, \"training\"), (X_valid, y_valid, \"validation\")], [(xplot, yplot, \"model\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the linear model does not seem to fit the data particularly well.  Let's see if we can improve this with a polynomial model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Building a Polynomial Regression Model in Scikit-Learn \n",
    "***\n",
    "\n",
    "Recall that Polynomial Regression is just multiple linear regression where we create new features that are powers of our original single feature.  In other words, our single feature $X$ turns into multiple features as \n",
    "\n",
    "$$\n",
    "    X \\quad \\mapsto \\quad X_1 = X, \\quad X_2 = X^2, \\quad X_3 = X^3 ,\\quad \\ldots,\\quad X_p = X^p \n",
    "$$\n",
    "\n",
    "We could create a new Numpy array where each column is a power of our original feature $X$, but Scikit-Learn can do this much more efficiently using the [PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) class. We'll demonstrate it's functionality first on a very simple example and then apply it to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "Z = np.array([[1], [2], [3]])\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `Z` is just a 2D Numpy array with $1$, $2$, and $3$ as the entries in it's sole column.  Next we'll create an instance of PolynomialFeatures that augments `Z` with polynomial features up to degree 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cubic_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "Zp = cubic_features.fit_transform(Z)\n",
    "print(Zp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the resulting `Zp` array includes the square and cubes of the original vector as new columns.  Note that if you change the `include_bias` flag to `True` it will prepend a column of ones, making our new matrix a regression design matrix.  We will skip this though because most regression classes in Scikit-Learn will do this for us internally (this is the `fit_intercept=True` flag in `Ridge` described previously). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: OK, time to fit our polynomial regression model.  We'll do this by transforming our training set into a matrix of polynomial features, and then fitting a model using the `LinearRegression` class. For now, we'll start with a degree $3$ polynomial.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degree = 3 \n",
    "poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "Xp_train = poly_features.fit_transform(X_train)\n",
    "polyreg = LinearRegression()\n",
    "polyreg.fit(Xp_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Let's use our new model to make predictions on the training set and the validation set and computing the resulting MSEs. Note that our Polynomial Regression model expects a matrix of polynomial features, thus we have to transform our validation set features as well.  After that we'll make a plot of the resulting polynomial model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xp_valid = poly_features.transform(X_valid)\n",
    "yphat_train = polyreg.predict(Xp_train)\n",
    "yphat_valid = polyreg.predict(Xp_valid)\n",
    "print(\"Degree {:d} Train MSE:      {:.3f}\".format(degree, mean_squared_error(yphat_train, y_train)))\n",
    "print(\"Degree {:d} Validation MSE: {:.3f}\".format(degree, mean_squared_error(yphat_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xplot = np.linspace(-60,60,100).reshape(-1,1)\n",
    "xpplot = poly_features.transform(xplot)\n",
    "yplot = polyreg.predict(xpplot) \n",
    "dam_plot([(X_train, y_train, \"training\"), (X_valid, y_valid, \"validation\")], [(xplot, yplot, \"model\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Hey!  That looks pretty good! Notice that in addition to an aesthetically pleasing fit, both our training and validation errors went down.  \n",
    "\n",
    "But we know good things never last ... let's break it.  Go back to the previous part and increment the polynomial degree until the model starts overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Building a Polynomial Regression Pipeline in Scikit-Learn \n",
    "***\n",
    "\n",
    "OK, so our implementation of a polynomial regression model above had a lot of moving parts.  We had to transform both our training and validation data to include polynomial features and then fit a regression model.  Eventually we'll add more bells and whistles to our model as well.  At this point, it's a good idea to talk about how to consolidate this process into a single command using Scikit-Learn's [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class.  \n",
    "\n",
    "The general idea is that we'll include all of our transformations and models into a pipeline so we can perform a single call to fit and predict to do the magic.  \n",
    "\n",
    "**Part A**: Here is how you build a pipeline to generate polynomial features and then fit a regression model at the same time.  Note that you've seen all of these routines before, we're just combining them into one. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "degree = 3\n",
    "polycombo = [(\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "                (\"linear_regression\", LinearRegression())]\n",
    "polypipe = Pipeline(polycombo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we call things like `.fit` and `.predict` on our pipeline object, it knows to perform the `.fit` and `.predict` methods for each of the constituents parts in sequence.  Let's try it. Notice that the output after calling `.fit` gives you a nice summary of all of the methods and parameters in your pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polypipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll use our pipeline to make predictions, check MSEs, and produce a plot.  The workflow will pretty much the same as before, but we've consolidated all of the individual calls to the tranformers and model into the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yphat_train = polypipe.predict(X_train)\n",
    "yphat_valid = polypipe.predict(X_valid)\n",
    "print(\"Degree {:d} Train MSE:      {:.3f}\".format(degree, mean_squared_error(yphat_train, y_train)))\n",
    "print(\"Degree {:d} Validation MSE: {:.3f}\".format(degree, mean_squared_error(yphat_valid, y_valid)))\n",
    "\n",
    "xplot = np.linspace(-60,60,100).reshape(-1,1)\n",
    "yplot = polypipe.predict(xplot) \n",
    "dam_plot([(X_train, y_train, \"training\"), (X_valid, y_valid, \"validation\")], [(xplot, yplot, \"model\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Let's see how we can systematically zero in on a good polynomial degree. Typically what we'd do is run our model for many values plynomial degree and examine the MSE on the training and validation sets.  The optimal polynomial degree is then the place on the curve with the lowest validation error.  Scikit-Learn implements this functionality using something called a [validation_curve](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html#sklearn.model_selection.validation_curve). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Evaluate the model for degrees 1 through 10 \n",
    "degrees = np.array(range(1,10+1))\n",
    "\n",
    "neg_MSE_train_folds, neg_MSE_valid_folds = validation_curve(polypipe, X, y, \n",
    "                                                            param_name=\"poly__degree\", param_range=degrees,\n",
    "                                                            cv=10, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "MSE_train = -np.mean(neg_MSE_train_folds, axis=1)\n",
    "MSE_valid = -np.mean(neg_MSE_valid_folds, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,6))\n",
    "ax.plot(degrees, MSE_train, lw=3, color=\"steelblue\", label=\"training\")\n",
    "ax.plot(degrees, MSE_valid, lw=3, color=\"green\", label=\"validation\")\n",
    "ax.grid(alpha=0.25)\n",
    "ax.set_xlabel(\"polynomial degree\", fontsize=12)\n",
    "ax.set_ylabel(\"Error\", fontsize=12)\n",
    "ax.legend(loc=\"upper right\", fontsize=16);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Which value of the polynomial degree appears to be optimal from our validation study? (Also, what's going on with the see-saw pattern for high polynomial degrees?)  Create a new polynomial regression model using the desired polynomial degree, check MSE, make plots, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "degree = 1 # TODO - Insert best degree here \n",
    "best_combo= [(\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "            (\"linear_regression\", LinearRegression())]\n",
    "bestpolypipe = Pipeline(best_combo)\n",
    "\n",
    "bestpolypipe.fit(X_train, y_train)\n",
    "yphat_train = bestpolypipe.predict(X_train)\n",
    "yphat_valid = bestpolypipe.predict(X_valid)\n",
    "print(\"Degree {:d} Train MSE:      {:.3f}\".format(degree, mean_squared_error(yphat_train, y_train)))\n",
    "print(\"Degree {:d} Validation MSE: {:.3f}\".format(degree, mean_squared_error(yphat_valid, y_valid)))\n",
    "\n",
    "xplot = np.linspace(-60,60,100).reshape(-1,1)\n",
    "yplot = bestpolypipe.predict(xplot) \n",
    "dam_plot([(X_train, y_train, \"training\"), (X_valid, y_valid, \"validation\")], [(xplot, yplot, \"model\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Evaluating the Bias-Variance using Learning Curves \n",
    "***\n",
    "\n",
    "We've now constructed several different models: a vanilla linear regression model (`linreg`), a general polynomial model with various degrees (`polypipe`), and a polynomial regression model with our empirically determined best degree (`bestpolypipe`). \n",
    "\n",
    "**Part A**: What does your intuition tell you with regard to the Bias-Variance Trade-Off of these models.  Does each of them have high or low Bias? Does each of them have high or low variance? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: One way to estimate the bias-variance of a real-life model is by plotting a so-called learning curve.  This is how you plot a learning curve for the `linreg` model in Scikit-Learn. What does the resulting learning curve indicate with respect to bias-variance for the linear model?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, neg_MSE_train_folds, neg_MSE_valid_folds = learning_curve(estimator=linreg, X=X, y=y,\n",
    "                                                        train_sizes=np.linspace(0.1, 1, 20), cv=10,\n",
    "                                                        scoring=\"neg_mean_squared_error\") \n",
    "\n",
    "MSE_train = -np.mean(neg_MSE_train_folds, axis=1)\n",
    "MSE_valid = -np.mean(neg_MSE_valid_folds, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,6))\n",
    "ax.plot(train_sizes, MSE_train, lw=3, color=\"steelblue\", label=\"training\")\n",
    "ax.plot(train_sizes, MSE_valid, lw=3, color=\"green\", label=\"validation\")\n",
    "ax.grid(alpha=0.25)\n",
    "ax.set_xlabel(\"training set size\", fontsize=16)\n",
    "ax.set_ylabel(\"Error\", fontsize=16)\n",
    "ax.legend(loc=\"upper right\", fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Repeat **Part B** for the degree 10 model `polypipe`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The validation curve function messed with the polynomial \n",
    "# degree of this model, so let's set it to 10 explicitly \n",
    "polypipe.set_params(poly__degree=10)\n",
    "\n",
    "train_sizes = np.array([13+ii for ii in range(43-13)])\n",
    "train_sizes, neg_MSE_train_folds, neg_MSE_valid_folds = learning_curve(estimator=polypipe, X=X, y=y,\n",
    "                                                        train_sizes=train_sizes, cv=5,\n",
    "                                                        scoring=\"neg_mean_squared_error\") \n",
    "\n",
    "MSE_train = -np.mean(neg_MSE_train_folds, axis=1)\n",
    "MSE_valid = -np.mean(neg_MSE_valid_folds, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,6))\n",
    "ax.plot(train_sizes, MSE_train, lw=3, color=\"steelblue\", label=\"training\")\n",
    "ax.plot(train_sizes, MSE_valid, lw=3, color=\"green\", label=\"validation\")\n",
    "ax.grid(alpha=0.25)\n",
    "ax.set_xlabel(\"training set size\", fontsize=16)\n",
    "ax.set_ylabel(\"Error\", fontsize=16)\n",
    "ax.legend(loc=\"upper right\", fontsize=16);\n",
    "ax.set_ylim([0,200]); # Comment this line out if you want to see how bad it really is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Repeat **Part B** for the optimal degree polynomial model `bestpolypipe`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes = np.array([13+ii for ii in range(43-13)])\n",
    "train_sizes, neg_MSE_train_folds, neg_MSE_valid_folds = learning_curve(estimator=bestpolypipe, X=X, y=y,\n",
    "                                                        train_sizes=np.linspace(.28, 1, 20), cv=5, \n",
    "                                                        scoring=\"neg_mean_squared_error\") \n",
    "\n",
    "MSE_train = -np.mean(neg_MSE_train_folds, axis=1)\n",
    "MSE_valid = -np.mean(neg_MSE_valid_folds, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,6))\n",
    "ax.plot(train_sizes, MSE_train, lw=3, color=\"steelblue\", label=\"training\")\n",
    "ax.plot(train_sizes, MSE_valid, lw=3, color=\"green\", label=\"validation\")\n",
    "ax.grid(alpha=0.25)\n",
    "ax.set_xlabel(\"training set size\", fontsize=16)\n",
    "ax.set_ylabel(\"Error\", fontsize=16)\n",
    "ax.legend(loc=\"upper right\", fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dam_plot(scatter=[], models=[]):\n",
    "    '''\n",
    "    Function to plot the dam data \n",
    "    '''\n",
    "    \n",
    "    # colors for scatter plots and model plots \n",
    "    scolors = [\"steelblue\", \"#a76c6e\", \"#6a9373\", \"orange\"]\n",
    "    mcolors = [\"black\", \"gray\"]\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,6))\n",
    "    \n",
    "    # Loop over scatter data and make plots \n",
    "    for ii, (x, y, label) in enumerate(scatter):\n",
    "        ax.scatter(x, y, s=100, color=scolors[ii], label=label, zorder=2)\n",
    "        \n",
    "    # Loop over model data and make plots \n",
    "    for ii, (xplot, yplot, label) in enumerate(models):\n",
    "        ax.plot(xplot, yplot, color=mcolors[ii], lw=3, label=label, zorder=1)\n",
    "        \n",
    "    # Set axis limits\n",
    "    ax.set_xlim([-60,60])\n",
    "    ax.set_ylim([-5,60])\n",
    "        \n",
    "    # Label all the things \n",
    "    ax.set_xlabel(\"change in water level\", fontsize=16)\n",
    "    ax.set_ylabel(\"water flowing out of damn\", fontsize=16)\n",
    "    ax.set_title(\"Dam Data\", fontsize=20); ax.grid(alpha=0.25)\n",
    "    ax.legend(loc=\"upper left\", fontsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

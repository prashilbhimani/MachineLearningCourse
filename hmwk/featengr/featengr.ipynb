{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Homework \n",
    "***\n",
    "**Name**: **Prashil Bhimani**\n",
    "\n",
    "**Kaggle Username**: **prashilbhimani**\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday February 23rd**. Additionally, you must make at least one submission to the **Kaggle** competition before it closes at **4:59pm on Friday February 23rd**. Submit only this Jupyter notebook to Moodle. Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "When people are discussing popular media, there’s a concept of spoilers. That is, critical information about the plot of a TV show, book, or movie that “ruins” the experience for people who haven’t read / seen it yet.\n",
    "\n",
    "The goal of this assignment is to do text classification on forum posts from the website [tvtropes.org](http://tvtropes.org/), to predict whether a post is a spoiler or not. We'll be using the logistic regression classifier provided by sklearn.\n",
    "\n",
    "Unlike previous assignments, the code provided with this assignment has all of the functionality required. Your job is to make the functionality better by improving the features the code uses for text classification.\n",
    "\n",
    "**NOTE**: Because the goal of this assignment is feature engineering, not classification algorithms, you may not change the underlying algorithm or it's parameters\n",
    "\n",
    "This assignment is structured in a way that approximates how classification works in the real world: Features are typically underspecified (or not specified at all). You, the data digger, have to articulate the features you need. You then compete against others to provide useful predictions.\n",
    "\n",
    "It may seem straightforward, but do not start this at the last minute. There are often many things that go wrong in testing out features, and you'll want to make sure your features work well once you've found them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle In-Class Competition \n",
    "***\n",
    "\n",
    "In addition to turning in this notebook on Moodle, you'll also need to submit your predictions on Kaggle, an online tournament site for machine learning competitions. The competition page can be found here:  \n",
    "\n",
    "[https://www.kaggle.com/c/feature-engineering-csci-5622-spring-2018](https://www.kaggle.com/c/feature-engineering-csci-5622-spring-2018)\n",
    "\n",
    "Additionally, a private invite link for the competition has been posted to Piazza. \n",
    "\n",
    "The starter code below has a `model_predict` method which produces a two column CSV file that is correctly formatted for Kaggle (predictions.csv). It should have the example Id as the first column and the prediction (`True` or `False`) as the second column. If you change this format your submissions will be scored as zero accuracy on Kaggle. \n",
    "\n",
    "**Note**: You may only submit **THREE** predictions to Kaggle per day.  Instead of using the public leaderboard as your sole evaluation processes, it is highly recommended that you perform local evaluation using a validation set or cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 1: Feature Engineering \n",
    "***\n",
    "\n",
    "The `FeatEngr` class is where the magic happens.  In it's current form it will read in the training data and vectorize it using simple Bag-of-Words.  It then trains a model and makes predictions.  \n",
    "\n",
    "25 points of your grade will be generated from your performance on the the classification competition on Kaggle. The performance will be evaluated on accuracy on the held-out test set. Half of the test set is used to evaluate accuracy on the public leaderboard.  The other half of the test set is used to evaluate accuracy on the private leaderboard (which you will not be able to see until the close of the competition). \n",
    "\n",
    "You should be able to significantly improve on the baseline system (i.e. the predictions made by the starter code we've provided) as reported by the Kaggle system.  Additionally, the top **THREE** students from the **PRIVATE** leaderboard at the end of the contest will receive 5 extra credit points towards their Problem 1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class QuoteVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"this would allow us to fit the model based on the X input.\"\"\"\n",
    "        return self\n",
    "    def transform(self, examples,y=None):\n",
    "        value=[]\n",
    "        for i in examples :\n",
    "            value.append(i.count('\"')/len(i.split(\" \")))\n",
    "            \n",
    "        value=np.rot90(np.asmatrix(value))\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class POSVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"this would allow us to fit the model based on the X input.\"\"\"\n",
    "        return self\n",
    "    def transform(self, examples,y=None):\n",
    "        value=[]\n",
    "        import nltk\n",
    "        from nltk import pos_tag, word_tokenize\n",
    "        for i in examples :\n",
    "            text = word_tokenize(i)\n",
    "            a=nltk.pos_tag(text)\n",
    "            noun_count=0\n",
    "            verb_count=0\n",
    "            for x in a:\n",
    "                if x[1]==\"NN\" or x[1]==\"PP\":\n",
    "                    noun_count=noun_count+1\n",
    "                if x[1]==\"VB\" or x[1]==\"RB\":\n",
    "                    verb_count=verb_count+1\n",
    "            value.append([noun_count/len(a),verb_count/len(a)])\n",
    "            \n",
    "        value=np.asmatrix(value)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class SentenceSentimentVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"this would allow us to fit the model based on the X input.\"\"\"\n",
    "        return self\n",
    "    def transform(self, examples,y=None):\n",
    "        value=[]\n",
    "        from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        for i in examples :\n",
    "            value.append(sid.polarity_scores(i)[\"compound\"])\n",
    "        value=np.rot90(np.asmatrix(value))\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_punctuation(text):\n",
    "    punctuation_numbers_specialcharacters = re.compile(r'[^a-zA-Z ]')\n",
    "    text = punctuation_numbers_specialcharacters.sub('', text)\n",
    "    return text.lower()\n",
    "class FeatEngr:\n",
    "    def __init__(self):\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "        from sklearn.pipeline import FeatureUnion,Pipeline\n",
    "        from sklearn.feature_selection import SelectPercentile,SelectKBest, f_classif\n",
    "        \n",
    "        self.one_gram_vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                                          binary=False,\n",
    "                                          decode_error='strict',\n",
    "                                          encoding='utf-8',\n",
    "                                          input='content',\n",
    "                                          lowercase=True,\n",
    "                                          max_df=1.0,\n",
    "                                          preprocessor=remove_punctuation,\n",
    "                                          max_features=None,\n",
    "                                          min_df=1,\n",
    "                                          stop_words='english',\n",
    "                                          ngram_range=(1, 1),\n",
    "                                          strip_accents=None,\n",
    "                                          token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "                                          vocabulary=None,\n",
    "                                          use_idf=True)\n",
    "        self.two_gram_vectorizer=TfidfVectorizer(analyzer='word', \n",
    "                                          binary=False,\n",
    "                                          decode_error='strict',\n",
    "                                          encoding='utf-8',\n",
    "                                          input='content',\n",
    "                                          lowercase=True,\n",
    "                                          max_df=1.0,\n",
    "                                          max_features=None,\n",
    "                                          min_df=1,\n",
    "                                          ngram_range=(2, 2),\n",
    "                                          strip_accents=None,\n",
    "                                          token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "                                          vocabulary=None,\n",
    "                                          use_idf=True)\n",
    "        self.trope_vectorizer = CountVectorizer()\n",
    "        self.pos_vectorizer=POSVectorizer()\n",
    "        self.feature_selector = SelectPercentile(f_classif, percentile=80)\n",
    "        \n",
    "        self.allmyfeatures = FeatureUnion([\n",
    "            ('subject1', Pipeline([\n",
    "                ('selector', ItemSelector(key='sentence')),\n",
    "                ('tfidf', self.one_gram_vectorizer),\n",
    "            ])),\n",
    "            ('subject2', Pipeline([\n",
    "                ('selector', ItemSelector(key='sentence')),\n",
    "                ('tfidf2', self.two_gram_vectorizer),\n",
    "            ])),\n",
    "            ('subject3', Pipeline([\n",
    "                ('selector', ItemSelector(key='trope')),\n",
    "                ('TropeVectorizer',self.trope_vectorizer) ,\n",
    "            ])),\n",
    "             ('subject4', Pipeline([\n",
    "                 ('selector', ItemSelector(key='trope')),\n",
    "                 ('POSVectorizer', self.pos_vectorizer),\n",
    "             ]))\n",
    "        ])\n",
    "        dfTrain = pd.read_csv(\"../data/spoilers/train.csv\")\n",
    "        x=dfTrain\n",
    "        \n",
    "        self.train_data=dfTrain.drop(['spoiler'], axis=1)\n",
    "        self.train_label=np.asarray(x.drop(['sentence','trope','page'],axis=1)).ravel()\n",
    "        \n",
    "        dfTest  = pd.read_csv(\"../data/spoilers/test.csv\")\n",
    "        self.test_data=dfTest.drop(['page'], axis=1)\n",
    "        \n",
    "    def plot_learning_curve(self,title,ylim=None, cv=10,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.datasets import load_digits\n",
    "        from sklearn.model_selection import learning_curve\n",
    "        from sklearn.model_selection import ShuffleSplit\n",
    "       \n",
    "        estimator=self.logreg\n",
    "        X=self.X_train\n",
    "        y=self.train_label\n",
    "        plt.figure()\n",
    "        plt.title(title)\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.xlabel(\"Training examples\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "        test_scores_std = np.std(test_scores, axis=1)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "\n",
    "        plt.legend(loc=\"best\")\n",
    "        return plt\n",
    "                \n",
    "    \n",
    "    def tokenize(self,text):\n",
    "        import nltk\n",
    "        import string\n",
    "        from nltk.corpus import wordnet\n",
    "        text=text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        stemmed = []\n",
    "        for item in tokens :\n",
    "            #if wordnet.synsets(item):\n",
    "            stemmed.append(self.stemmer.stem(item))\n",
    "        return stemmed        \n",
    "\n",
    "    def build_train_features(self,data,label):\n",
    "        \"\"\"\n",
    "        Method to take in training text features and do further feature engineering \n",
    "        Most of the work in this homework will go here, or in similar functions  \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "        return self.allmyfeatures.fit_transform(data,label)\n",
    "\n",
    "    def get_test_features(self,data):\n",
    "        \"\"\"\n",
    "        Method to take in test text features and transform the same way as train features \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "        return self.allmyfeatures.transform(data)\n",
    "    \n",
    "    def show_top10(self):\n",
    "        \"\"\"\n",
    "        prints the top 10 features for the positive class and the \n",
    "        top 10 features for the negative class. \n",
    "        \"\"\"\n",
    "        feature_names = list(self.one_gram_vectorizer.get_feature_names()+self.two_gram_vectorizer.get_feature_names()+self.trope_vectorizer.get_feature_names()+[\"num_noun\",\"num_verb\"])\n",
    "        selected_features=self.feature_selector.get_support()\n",
    "        features=[]\n",
    "        for i in zip(feature_names,selected_features):\n",
    "            if i[1]:\n",
    "                features.append(str(i[0]))\n",
    "        \n",
    "        features=np.asarray(features)\n",
    "        top10 = np.argsort(self.logreg.coef_[0])[-10:]\n",
    "        bottom10 = np.argsort(self.logreg.coef_[0])[:10]\n",
    "        print(\"Pos: %s\" % \" \".join(features[top10]))\n",
    "        print(\"Neg: %s\" % \" \".join(features[bottom10]))\n",
    "    def train_model(self, random_state=1234):\n",
    "        \"\"\"\n",
    "        Method to read in training data from file, and \n",
    "        train Logistic Regression classifier. \n",
    "        \n",
    "        :param random_state: seed for random number generator \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        from sklearn.linear_model import LogisticRegression \n",
    "        \n",
    "        self.X_train = self.build_train_features(self.train_data,self.train_label)\n",
    "        self.X_train = self.feature_selector.fit_transform(self.X_train, self.train_label)\n",
    "        # train logistic regression model.  !!You MAY NOT CHANGE THIS!!\n",
    "        self.logreg = LogisticRegression(random_state=random_state)\n",
    "        \n",
    "        from sklearn.model_selection import KFold\n",
    "        kf = KFold(n_splits=10, random_state=None, shuffle=True)            \n",
    "        scores_list = [self.logreg.fit(self.X_train[train], self.train_label[train]).score(self.X_train[test], self.train_label[test] )\n",
    "            for train, test in kf.split(self.X_train)]\n",
    "        print(\"scores list:\" +str(scores_list))\n",
    "        print(\"mean:\" ,np.mean(scores_list))\n",
    "        self.logreg.fit(self.X_train, self.train_label)\n",
    "    def model_predict(self):\n",
    "        \"\"\"\n",
    "        Method to read in test data from file, make predictions\n",
    "        using trained model, and dump results to file \n",
    "        \"\"\"\n",
    "        # featurize test data \n",
    "        self.X_test = self.get_test_features(self.test_data)\n",
    "        self.X_test=self.feature_selector.transform(self.X_test)\n",
    "         # make predictions on test data \n",
    "        pred = self.logreg.predict(self.X_test)\n",
    "        #self.show_misclassified()\n",
    "        # dump predictions to file for submission to Kaggle  \n",
    "        pd.DataFrame({\"spoiler\": np.array(pred, dtype=bool)}).to_csv(\"prediction.csv\", index=True, index_label=\"Id\")\n",
    "    def error_analysis(self):\n",
    "        report_step=10\n",
    "        j=0\n",
    "        # featurize test data \n",
    "        features = self.get_test_features(self.train_data)\n",
    "        features= self.feature_selector.transform(features)\n",
    "        # make predictions on test data \n",
    "        pred = self.logreg.predict(features)\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i]!=self.train_label[i]:\n",
    "                if j%report_step==0:\n",
    "                    print(self.train_data['sentence'][i])\n",
    "                    print(\"True label \"+str(self.train_label[i]))\n",
    "                    print(\"Predictied label \"+str(pred[i]))\n",
    "                j=j+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores list:[0.81620718462823727, 0.80952380952380953, 0.82456140350877194, 0.8128654970760234, 0.82873851294903922, 0.81370091896407681, 0.80701754385964908, 0.81453634085213034, 0.80952380952380953, 0.82539682539682535]\n",
      "mean: 0.816207184628\n",
      "Pos: foreshadowing actually killed xanatosgambit death kills finale revealed dies whamepisode\n",
      "Neg: the show catchphrase one episode tim deadpansnarker usually drew domcom spinoff cory\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvFmYmVlHPwHeeSs3A2Ey4AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c412e5dc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate the FeatEngr clas \n",
    "feat = FeatEngr()\n",
    "\n",
    "#feat.explore_data1()\n",
    "# Train your Logistic Regression classifier \n",
    "feat.train_model(random_state=1200)\n",
    "\n",
    "# Shows the top 10 features for each class \n",
    "feat.show_top10()\n",
    "feat.plot_learning_curve(\"curve\")\n",
    "#feat.error_analysis()\n",
    "# Make prediction on test data and produce Kaggle submission file \n",
    "feat.model_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 2: Motivation and Analysis \n",
    "***\n",
    "\n",
    "The job of the written portion of the homework is to convince the grader that:\n",
    "\n",
    "- Your new features work\n",
    "- You understand what the new features are doing\n",
    "- You had a clear methodology for incorporating the new features\n",
    "\n",
    "Make sure that you have examples and quantitative evidence that your features are working well. Be sure to explain how you used the data (e.g., did you have a validation set? did you do cross-validation?) and how you inspected the results. In addition, it is very important that you show some kind of an **error analysis** throughout your process.  That is, you should demonstrate that you've looked at misclassified examples and put thought into how you can craft new features to improve your model. \n",
    "\n",
    "A sure way of getting a low grade is simply listing what you tried and reporting the Kaggle score for each. You are expected to pay more attention to what is going on with the data and take a data-driven approach to feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer\n",
    "*** \n",
    "\n",
    "To add new features I wrote a function named explore_data() (See code above)r various features I was able to get the following features\n",
    "\n",
    "1. N-Grams\n",
    "\n",
    "2. Tropes\n",
    "\n",
    "3. Parts of Speech\n",
    "\n",
    "4. Number of Quotes\n",
    "\n",
    "5. Sentiment of sentences\n",
    "\n",
    "Out of these features 1, 2 and 3 perform very well on the both the Train and Test samples. Feature 4 works well on the Train data but not on the test data. Feature 5  did not work well on the test or the train data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 1 - N-Grams\n",
    "\n",
    "One of the 1st papers I read was by Prof. Jordan Graber while working on the same dataset. He gave a pretty nice explanation about how bigarms describe transitive verbs such has \"to kill\" etc. To capture these kinds of transitive verbs we had to make sure that we do not remove stop words because that may lead to loss of such important information.\n",
    "\n",
    "But while analyzing 1-grams it is important to remove the stop words as they will not have any value on their own without the verbs connecting them. Also logically it would make sense to have stemming to merge words such as \"kill, kills and killed\". But after doing some error analysis on these it turns out that words in past tense like \"killed\" were not usually seen in spoiler but words like \"kills\" which are based in future or present tense will indicate that they are spoilers. Hence stemming was not a good idea. \n",
    "\n",
    "\n",
    "Also instead of the basic CountVectorizer used in baseline it was important to use something that would take into considreation the overall importance over all the sentences. Hence I used TFIDVectorizer.\n",
    "\n",
    "Reading more about TFIDVectorizer, I came across using_idf parameter which negates the importance of long sentences.\n",
    "\n",
    "Hence with these observations I developed 2 feature extractors one with TFIDVectorizer for 1-grams which will remove stop words and other 2-grams which will have almost no preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 2 - Tropes\n",
    "\n",
    "As defined by TVTropes.com are \"A trope is a storytelling device or convention, a shortcut for describing situations the storyteller can reasonably assume the audience will recognize.\" \n",
    "\n",
    "These are meant to describe some common types of things that happen in a TV Show. So a particular type of event across various shows will given a particular trope. \n",
    "\n",
    "So accordingly if a particular trope is related to spoilers then we can assume that there some other sentence of the same trope will also be a spoiler.\n",
    "\n",
    "Tropes are treated as categorical data.\n",
    "\n",
    "To use the Trope as a feature I converted it using a CountVectorizer. OneHotEncoding is will also give the same result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 3 - Parts of Speech\n",
    "\n",
    "After doing error analysis using the function I wrote about I found that most of the Spoilers have more number of nouns and verbs as compared to non spoiler statements. This logically makes sens becuase to describe a plot you will have to name characters and describe a particular action. These are the main examples of nouns and verbs. \n",
    "\n",
    "So I added 2 features namesly num_nouns and num_verbs. But one cavet was that the larger the sentence the more noun and verbs it will have. So I had to normalize the count by the length of the sentence.\n",
    "\n",
    "After adding that feature there was about 2% increase in accuracy and hence I was able to determine that it was a good feature.\n",
    "\n",
    "I wrote a new class POSVectorizer that is used in feature union.\n",
    "\n",
    "The class uses nltk to pos-tagger to count the number of noun and verbs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average nouns per sentence in spoiler 0.109515945546\n",
      "Average verb per sentence in spoiler 0.0844051991425\n",
      "Average noun per sentence in spoiler 0.115856269944\n",
      "Average verb per sentence in non-spoiler 0.078719678245\n"
     ]
    }
   ],
   "source": [
    "def explore_data_sentiment():\n",
    "    dfTrain = pd.read_csv(\"../data/spoilers/train.csv\")\n",
    "    examples=list(dfTrain[\"sentence\"])\n",
    "    spoiler=list(dfTrain[\"spoiler\"])\n",
    "    page_dict={}\n",
    "    value=[]\n",
    "    import nltk\n",
    "    from nltk import pos_tag, word_tokenize\n",
    "    for i in examples :\n",
    "        text = word_tokenize(i)\n",
    "        a=nltk.pos_tag(text)\n",
    "        noun_count=0\n",
    "        verb_count=0\n",
    "        for x in a:\n",
    "            if x[1]==\"NN\":\n",
    "                noun_count=noun_count+1\n",
    "            if x[1]==\"VB\" or x[1]==\"RB\":\n",
    "                verb_count=verb_count+1\n",
    "        value.append([noun_count/len(a),verb_count/len(a)])\n",
    "    #plt.plot(value,spoiler,\"ro\" )\n",
    "    noun_in_spoiler=[]\n",
    "    verb_in_spoiler=[]\n",
    "    noun_in_not_spoiler=[]\n",
    "    verb_in_not_spoiler=[]\n",
    "    num_spoiler=0\n",
    "    for i in zip(value,spoiler):\n",
    "        if i[1]:\n",
    "            noun_in_spoiler.append(i[0][0])\n",
    "            verb_in_spoiler.append(i[0][1])\n",
    "        else:\n",
    "            noun_in_not_spoiler.append(i[0][0])\n",
    "            verb_in_not_spoiler.append(i[0][1])\n",
    "    print(\"Average nouns per sentence in spoiler \",end=\"\")\n",
    "    print(np.mean(noun_in_spoiler))\n",
    "    print(\"Average verb per sentence in spoiler \",end=\"\")\n",
    "    print(np.mean(verb_in_spoiler))\n",
    "    print(\"Average noun per sentence in spoiler \",end=\"\")\n",
    "    print(np.mean(noun_in_not_spoiler))\n",
    "    print(\"Average verb per sentence in non-spoiler \",end=\"\")\n",
    "    print(np.mean(verb_in_not_spoiler))\n",
    "explore_data_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 4 - Number of Quotes\n",
    "\n",
    "After doing further research on TVTropes.org I came across this page : http://tvtropes.org/pmwiki/pmwiki.php/Main/Spoiler which gave me a hint that spoilerly sentences may have some quotes by the characters and hence I decided to add a new feature number of quotes. \n",
    "\n",
    "So I wrote a class QuotesVectorizer where I counted the number of quotes in a sentence and divided it by 2 as that 1 dalogue needs 2 quotes. But a similar cavet was that the larger the sentence the more chances of having a quote. So I had to normalize the count by the length of the sentence.\n",
    "\n",
    "After adding that feature there was about 1% increase in accuracy and hence I was able to determine that it was a good feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.427480916031\n",
      "0.41781063006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "def explore_data_quotes():\n",
    "    dfTrain = pd.read_csv(\"../data/spoilers/train.csv\")\n",
    "    examples=list(dfTrain[\"sentence\"])\n",
    "    spoiler=list(dfTrain[\"spoiler\"])\n",
    "    page_dict={}\n",
    "    value=[]\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    for i in examples :\n",
    "        value.append(i.count('\"'))\n",
    "    value=np.rot90(np.asmatrix(value))\n",
    "    quotes_in_spoiler=[]\n",
    "    quotes_in_not_spoiler=[]\n",
    "    num_spoiler=0\n",
    "    for i in zip(value,spoiler):\n",
    "        if i[1]:\n",
    "            quotes_in_spoiler.append(i[0])\n",
    "        else:\n",
    "            quotes_in_not_spoiler.append(i[0])\n",
    "            \n",
    "    print(np.mean(quotes_in_spoiler))\n",
    "    print(np.mean(quotes_in_not_spoiler))\n",
    "   \n",
    "explore_data_quotes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 5 - Sentiment of sentences\n",
    "\n",
    "Reading on in TVTropes there is a particular statement they had about spoilers being sentences that overzealous users have posted. So after doing some further digging in the data set I found that some spoilers have a lot of strong emotions like \"They killed him\" and so on. So I decided to have a new feature that could extract this. Hence I decided to analyse the sentiment of a sentence and decide if that will make a good feature.\n",
    "\n",
    "So nltk has a \"Vader\" corpus which can be used to analyze the sentiment of a statement. I focused on postive and negative sentiments rather than neutral sentiments. \n",
    "\n",
    "These features did not give a huge bump in training accuracy and also test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAOYLT67h2HcaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c412f69e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def explore_data_sentiment():\n",
    "    dfTrain = pd.read_csv(\"../data/spoilers/train.csv\")\n",
    "    examples=list(dfTrain[\"sentence\"])\n",
    "    spoiler=list(dfTrain[\"spoiler\"])\n",
    "    page_dict={}\n",
    "    value=[]\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    for i in examples :\n",
    "        value.append(sid.polarity_scores(i)[\"pos\"])\n",
    "    value=np.rot90(np.asmatrix(value))\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(value[:500],spoiler[:500],\"ro\" )\n",
    "explore_data_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis, Cross validation and feature selection\n",
    "\n",
    "I wrote a function named error_analysis that would print the misclassified points in the training data. To get a better idea I did do both K-Fold cross validation and plotted a learning curve. There was a huge gap between the training curve and the validation curve. That indicated that I need more features. That could be acquired by using extra dataset (I had time to add genre and other extra features but wanted to get the best accuracy using the available dataset)\n",
    "\n",
    "My K-Fold score is about 81.25% which is about 9% higher than the test accuracy score which means there was not a lot of overfitting. To negate overfitting I eliminated 20% features which were not important. Most of the eliminated features were from 1 and 2 grams as the lower 20% would be words that do not occur frequently or might not be general spoilery terms and not eliminiating those would just lead to overfitting on my training data.\n",
    "\n",
    "I used SelectPercentile from scikit_learn to select the 80% of the best features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints \n",
    "***\n",
    "\n",
    "- Don't use all the data until you're ready. \n",
    "\n",
    "- Examine the features that are being used.\n",
    "\n",
    "- Do error analyses.\n",
    "\n",
    "- If you have questions that aren’t answered in this list, feel free to ask them on Piazza.\n",
    "\n",
    "### FAQs \n",
    "***\n",
    "\n",
    "> Can I heavily modify the FeatEngr class? \n",
    "\n",
    "Totally.  This was just a starting point.  The only thing you cannot modify is the LogisticRegression classifier.  \n",
    "\n",
    "> Can I look at TV Tropes?\n",
    "\n",
    "In order to gain insight about the data yes, however, your feature extraction cannot use any additional data (beyond what I've given you) from the TV Tropes webpage.\n",
    "\n",
    "> Can I use IMDB, Wikipedia, or a dictionary?\n",
    "\n",
    "Yes, but you are not required to. So long as your features are fully automated, they can use any dataset other than TV Tropes. Be careful, however, that your dataset does not somehow include TV Tropes (e.g. using all webpages indexed by Google will likely include TV Tropes).\n",
    "\n",
    "> Can I combine features?\n",
    "\n",
    "Yes, and you probably should. This will likely be quite effective.\n",
    "\n",
    "> Can I use Mechanical Turk?\n",
    "\n",
    "That is not fully automatic, so no. You should be able to run your feature extraction without any human intervention. If you want to collect data from Mechanical Turk to train a classifier that you can then use to generate your features, that is fine. (But that’s way too much work for this assignment.)\n",
    "\n",
    "> Can I use a Neural Network to automatically generate derived features? \n",
    "\n",
    "No. This assignment is about your ability to extract meaningful features from the data using your own experimentation and experience.\n",
    "\n",
    "> What sort of improvement is “good” or “enough”?\n",
    "\n",
    "If you have 10-15% improvement over the baseline (on the Public Leaderboard) with your features, that’s more than sufficient. If you fail to get that improvement but have tried reasonable features, that satisfies the requirements of assignment. However, the extra credit for “winning” the class competition depends on the performance of other students.\n",
    "\n",
    "> Where do I start?  \n",
    "\n",
    "It might be a good idea to look at the in-class notebook associated with the Feature Engineering lecture where we did similar experiments. \n",
    "\n",
    "\n",
    "> Can I use late days on this assignment? \n",
    "\n",
    "You can use late days for the write-up submission, but the Kaggle competition closes at **4:59pm on Friday February 23rd**\n",
    "\n",
    "> Why does it say that the competition ends at 11:59pm when the assignment says 4:59pm? \n",
    "\n",
    "The end time/date are in UTC.  11:59pm UTC is equivalent to 4:59pm MST.  Kaggle In-Class does not allow us to change this. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
